{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436de838-6efd-4279-a9c5-dc475ffa3f22",
   "metadata": {},
   "source": [
    "# CustomPaperPipeline: Advanced AI for Scientific Paper Generation\n",
    "\n",
    "by Nigel van der Laan, AI Researcher and Developer, ARQNXS\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the rapidly evolving landscape of artificial intelligence and natural language processing, we've developed a cutting-edge AI pipeline that pushes the boundaries of automated scientific paper generation. This innovative system, which we call the CustomPaperPipeline, combines advanced machine learning techniques with natural language processing to create a powerful tool for researchers and academics.\n",
    "\n",
    "## The Core Components\n",
    "\n",
    "Our CustomPaperPipeline is built on several key components:\n",
    "\n",
    "1. **Data Collection**: The system begins by fetching relevant scientific papers from arXiv, a popular repository for scientific preprints. This ensures that our model is trained on the latest research in the specified field.\n",
    "\n",
    "2. **Preprocessing**: Raw text data is cleaned and structured, extracting key sections such as abstracts, introductions, methods, results, and discussions. This step is crucial for maintaining the logical flow and structure of scientific papers.\n",
    "\n",
    "3. **Custom Transformer Model**: At the heart of our pipeline is a custom-built transformer model. This neural network architecture, inspired by state-of-the-art language models like GPT, is specifically tailored for scientific writing.\n",
    "\n",
    "4. **Training Process**: The model is trained on the preprocessed scientific papers, learning the patterns, structures, and language specific to academic writing in the chosen field.\n",
    "\n",
    "5. **Paper Generation**: Once trained, the model can generate new scientific papers based on given prompts, complete with proper section structure and academic language.\n",
    "\n",
    "6. **Refinement**: As an optional step, the generated papers can be refined using OpenAI's GPT model, adding an extra layer of polish and coherence.\n",
    "\n",
    "## Technical Innovations\n",
    "\n",
    "Several technical innovations make our CustomPaperPipeline stand out:\n",
    "\n",
    "### Custom Transformer Encoder\n",
    "\n",
    "We've implemented a custom transformer encoder layer that allows for more efficient processing of scientific text. The key innovation lies in our modification of the standard transformer architecture to better handle the unique characteristics of scientific writing.\n",
    "\n",
    "The core of our custom transformer encoder is defined as follows:\n",
    "\n",
    "```python\n",
    "class CustomTransformerEncoderLayer(OriginalTransformerEncoderLayer):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=F.relu,\n",
    "                 layer_norm_eps=1e-5, norm_first=False,\n",
    "                 device=None, dtype=None):\n",
    "        super().__init__(\n",
    "            d_model, nhead, dim_feedforward, dropout, activation,\n",
    "            layer_norm_eps, batch_first=True, norm_first=norm_first,\n",
    "            device=device, dtype=dtype\n",
    "        )\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "```\n",
    "\n",
    "This custom layer incorporates batch-first processing and allows for more flexible handling of scientific text structures.\n",
    "\n",
    "### Adaptive Tokenization\n",
    "\n",
    "Our system uses the GPT-2 tokenizer but adapts it for scientific vocabulary, ensuring better representation of domain-specific terms. The tokenization process can be represented by the following function:\n",
    "\n",
    "$$ T(x) = \\{t_1, t_2, ..., t_n\\} $$\n",
    "\n",
    "Where $T$ is the tokenization function, $x$ is the input text, and $\\{t_1, t_2, ..., t_n\\}$ is the sequence of tokens.\n",
    "\n",
    "### Dynamic Mask Handling\n",
    "\n",
    "The model intelligently handles attention masks, allowing it to focus on relevant parts of the input during both training and generation. The attention mechanism can be described by the following equation:\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
    "\n",
    "Where $Q$, $K$, and $V$ are the query, key, and value matrices respectively, and $d_k$ is the dimension of the key vectors.\n",
    "\n",
    "### Flexible Paper Structure\n",
    "\n",
    "Our paper formatting algorithm can adapt to various scientific paper structures, ensuring that generated papers follow standard academic conventions. The structure is maintained through a series of regex-based extractors:\n",
    "\n",
    "```python\n",
    "def extract_section(text: str, section_name: str) -> str:\n",
    "    pattern = f\"{section_name}[:.\\n](.*?)(?:\\n\\n|\\n(?=[0-9]+\\.?\\s+[A-Z]))\"\n",
    "    match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "```\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "The core of our system is the `TransformerModel` class, which implements a custom transformer architecture:\n",
    "\n",
    "```python\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads, dropout=0.1, device=None):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        encoder_layers = CustomTransformerEncoderLayer(hidden_size, num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        embedded = self.embedding(x)\n",
    "        if mask is not None:\n",
    "            mask = mask.bool()\n",
    "            mask = ~mask\n",
    "        output = self.transformer_encoder(embedded, src_key_padding_mask=mask)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "```\n",
    "\n",
    "The model uses an embedding layer, followed by multiple transformer encoder layers, and a final linear layer for output generation.\n",
    "\n",
    "## Training Process\n",
    "\n",
    "The training process involves minimizing the cross-entropy loss between the model's predictions and the actual tokens in the scientific papers. The loss function can be expressed as:\n",
    "\n",
    "$$ L = -\\frac{1}{N}\\sum_{i=1}^N \\sum_{j=1}^V y_{ij} \\log(p_{ij}) $$\n",
    "\n",
    "Where $N$ is the number of samples, $V$ is the vocabulary size, $y_{ij}$ is the true distribution, and $p_{ij}$ is the predicted probability distribution.\n",
    "\n",
    "## Paper Generation\n",
    "\n",
    "The paper generation process uses a combination of top-k and top-p (nucleus) sampling to produce diverse and coherent text. The sampling process can be described by the following algorithm:\n",
    "\n",
    "1. Compute the probability distribution over the vocabulary: $P(x_t | x_{<t})$\n",
    "2. Sort the probabilities in descending order\n",
    "3. Keep top-k tokens or tokens that cumulatively exceed probability p\n",
    "4. Renormalize the probabilities\n",
    "5. Sample from the reduced vocabulary\n",
    "\n",
    "This process is implemented in the `top_k_top_p_filtering` function:\n",
    "\n",
    "```python\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float('Inf')):\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "```\n",
    "\n",
    "## Potential Applications\n",
    "\n",
    "The CustomPaperPipeline has numerous potential applications in academia and research:\n",
    "\n",
    "1. **Literature Review Assistance**: Researchers can use the system to generate initial drafts of literature reviews, saving time in the early stages of research.\n",
    "2. **Hypothesis Generation**: By analyzing patterns in existing research, the system could suggest novel hypotheses for further investigation.\n",
    "3. **Writing Support**: The pipeline can assist researchers in overcoming writer's block by generating initial drafts or suggesting content for specific sections.\n",
    "4. **Educational Tool**: Students can use the system to understand the structure and language of scientific papers in their field of study.\n",
    "\n",
    "## Ethical Considerations\n",
    "\n",
    "While the CustomPaperPipeline represents a significant advancement in AI-assisted scientific writing, it's crucial to approach its use ethically. The system is designed as a tool to augment human researchers, not replace them. All generated content should be thoroughly reviewed, fact-checked, and appropriately edited by human experts before any form of publication or submission.\n",
    "\n",
    "## Future Directions\n",
    "\n",
    "As we continue to develop and refine the CustomPaperPipeline, we're exploring several exciting avenues for improvement:\n",
    "\n",
    "- Integration with citation databases for automatic reference generation\n",
    "- Expansion to cover a wider range of scientific disciplines\n",
    "- Implementation of more sophisticated coherence and factual consistency checks\n",
    "- Development of a user-friendly interface for easier adoption by researchers\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The CustomPaperPipeline represents a significant step forward in the application of AI to scientific research and writing. By automating certain aspects of the paper writing process, we aim to free up researchers' time for more creative and analytical tasks. As we continue to refine and expand this technology, we're excited about its potential to accelerate scientific discovery and enhance the productivity of researchers worldwide.\n",
    "\n",
    "We welcome collaborations and feedback from the scientific community as we work towards shaping the future of AI-assisted academic writing.\n",
    "\n",
    "---\n",
    "\n",
    "For more information or collaboration opportunities, please contact:\n",
    "\n",
    "Nigel van der Laan\n",
    "AI Researcher and Developer\n",
    "ARQNXS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a62324-f63d-4f17-bd07-8860ca47ad6c",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41198e29-1cb8-455e-ac48-592091f7ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "import lxml.etree as ET\n",
    "from typing import List, Dict, Union\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder\n",
    "from torch.nn.modules.transformer import TransformerEncoderLayer as OriginalTransformerEncoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import openai\n",
    "from multiprocessing import Pool\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "openai_key = 'sk-proj-xSYMrx4bYZr59123456789'\n",
    "\n",
    "class CustomTransformerEncoderLayer(OriginalTransformerEncoderLayer):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=F.relu,\n",
    "                 layer_norm_eps=1e-5, norm_first=False,\n",
    "                 device=None, dtype=None):\n",
    "        super().__init__(\n",
    "            d_model, nhead, dim_feedforward, dropout, activation,\n",
    "            layer_norm_eps, batch_first=True, norm_first=norm_first,\n",
    "            device=device, dtype=dtype\n",
    "        )\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "\n",
    "class PaperDataset(Dataset):\n",
    "    def __init__(self, data_file, tokenizer, max_length):\n",
    "        self.data = self.load_data(data_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def load_data(self, data_file):\n",
    "        with open(data_file, 'r', encoding='utf-8') as file:\n",
    "            data = [json.loads(line) for line in file]\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        paper = self.data[idx]\n",
    "        # Convert all values to strings, joining lists if necessary\n",
    "        paper_text = ' '.join(str(value) if isinstance(value, str) else ' '.join(value) for value in paper.values())\n",
    "        encoded = self.tokenizer(\n",
    "            paper_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return encoded['input_ids'].squeeze(), encoded['attention_mask'].squeeze()\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads, dropout=0.1, device=None):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        encoder_layers = CustomTransformerEncoderLayer(hidden_size, num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        embedded = self.embedding(x)\n",
    "        if mask is not None:\n",
    "            # Convert mask to boolean\n",
    "            mask = mask.bool()\n",
    "            # Invert the mask as per PyTorch convention\n",
    "            mask = ~mask\n",
    "        output = self.transformer_encoder(embedded, src_key_padding_mask=mask)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def generate(self, input_ids, attention_mask=None, max_length=100, num_return_sequences=1, temperature=1.0, no_repeat_ngram_size=2, pad_token_id=None):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        current_length = input_ids.shape[1]\n",
    "\n",
    "        for _ in range(max_length - current_length):\n",
    "            outputs = self(input_ids, mask=attention_mask)\n",
    "            next_token_logits = outputs[:, -1, :]\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
    "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = torch.cat([attention_mask, attention_mask.new_ones((batch_size, 1))], dim=-1)\n",
    "\n",
    "            if next_token.item() == pad_token_id:\n",
    "                break\n",
    "\n",
    "        return input_ids\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float('Inf')):\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "class CustomPaperPipeline:\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads, dropout=0.1, openai_api_key=None, cache_dir='cache'):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        \n",
    "        # Set the padding token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        \n",
    "        self.model = TransformerModel(len(self.tokenizer), hidden_size, num_layers, num_heads, dropout, device=self.device).to(self.device)\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        if openai_api_key:\n",
    "            openai.api_key = openai_api_key\n",
    "\n",
    "    def fetch_arxiv_paper(self, query: str, start: int, max_results: int = 100) -> str:\n",
    "        base_url = 'http://export.arxiv.org/api/query?'\n",
    "        search_query = f'search_query={query}&start={start}&max_results={max_results}'\n",
    "        response = requests.get(base_url + search_query)\n",
    "        return response.text\n",
    "\n",
    "    def save_papers(self, query: str, dir_path: str, max_results: int = 100, max_papers: int = 10000, num_processes: int = 4):\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "        \n",
    "        with Pool(processes=num_processes) as pool:\n",
    "            start_values = list(range(0, max_papers, max_results))\n",
    "            results = pool.starmap(self.fetch_arxiv_paper, [(query, start) for start in start_values])\n",
    "        \n",
    "        downloaded_papers = 0\n",
    "        for i, data in enumerate(results):\n",
    "            if '<entry>' not in data:\n",
    "                logging.info(f\"No more papers found after downloading {downloaded_papers} papers.\")\n",
    "                break\n",
    "            with open(f\"{dir_path}/papers_{i*max_results}.xml\", 'w', encoding='utf-8') as f:\n",
    "                f.write(data)\n",
    "            downloaded_papers += max_results\n",
    "        \n",
    "        logging.info(f\"Downloaded {downloaded_papers} papers.\")\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        text = re.sub(r'<.*?>+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def extract_title(self, text: str) -> str:\n",
    "        lines = text.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.strip() and not re.match(r'^(Abstract|Authors?|Keywords):', line, re.IGNORECASE):\n",
    "                return line.strip()\n",
    "        return \"\"\n",
    "\n",
    "    def extract_abstract(self, text: str) -> str:\n",
    "        abstract_pattern = r'Abstract[:.\\n](.*?)(?:\\n\\n|\\n(?=[0-9]+\\.?\\s+[A-Z]))'\n",
    "        abstract_match = re.search(abstract_pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if abstract_match:\n",
    "            return abstract_match.group(1).strip()\n",
    "        return \"\"\n",
    "\n",
    "    def extract_introduction(self, text: str) -> str:\n",
    "        intro_pattern = r'(?:Introduction|Background)[:.\\n](.*?)(?:\\n\\n|\\n(?=[0-9]+\\.?\\s+[A-Z]))'\n",
    "        intro_match = re.search(intro_pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if intro_match:\n",
    "            return intro_match.group(1).strip()\n",
    "        return \"\"\n",
    "\n",
    "    def extract_methods(self, text: str) -> str:\n",
    "        methods_pattern = r'(?:Methods|Methodology|Materials and Methods)[:.\\n](.*?)(?:\\n\\n|\\n(?=[0-9]+\\.?\\s+[A-Z]))'\n",
    "        methods_match = re.search(methods_pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if methods_match:\n",
    "            return methods_match.group(1).strip()\n",
    "        return \"\"\n",
    "\n",
    "    def extract_results(self, text: str) -> str:\n",
    "        results_pattern = r'(?:Results|Findings)[:.\\n](.*?)(?:\\n\\n|\\n(?=[0-9]+\\.?\\s+[A-Z]))'\n",
    "        results_match = re.search(results_pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if results_match:\n",
    "            return results_match.group(1).strip()\n",
    "        return \"\"\n",
    "\n",
    "    def extract_discussion(self, text: str) -> str:\n",
    "        discussion_pattern = r'(?:Discussion|Conclusion)[:.\\n](.*?)(?:\\n\\n|\\n(?=[0-9]+\\.?\\s+[A-Z]))'\n",
    "        discussion_match = re.search(discussion_pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if discussion_match:\n",
    "            return discussion_match.group(1).strip()\n",
    "        return \"\"\n",
    "\n",
    "    def extract_references(self, text: str) -> List[str]:\n",
    "        ref_pattern = r'(?:References|Bibliography)(.*?)(?:\\n\\n|\\Z)'\n",
    "        ref_match = re.search(ref_pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if ref_match:\n",
    "            ref_text = ref_match.group(1)\n",
    "            return [ref.strip() for ref in re.split(r'\\n\\s*\\n|\\[[0-9]+\\]', ref_text) if ref.strip()]\n",
    "        return []\n",
    "\n",
    "    def process_paper(self, raw_text: str) -> Dict[str, Union[str, List[str]]]:\n",
    "        paper = {\n",
    "            'title': self.extract_title(raw_text),\n",
    "            'abstract': self.extract_abstract(raw_text),\n",
    "            'introduction': self.extract_introduction(raw_text),\n",
    "            'methods': self.extract_methods(raw_text),\n",
    "            'results': self.extract_results(raw_text),\n",
    "            'discussion': self.extract_discussion(raw_text),\n",
    "            'references': self.extract_references(raw_text)\n",
    "        }\n",
    "        \n",
    "        for key in paper:\n",
    "            if isinstance(paper[key], str):\n",
    "                paper[key] = self.preprocess_text(paper[key])\n",
    "            elif isinstance(paper[key], list):\n",
    "                paper[key] = ' '.join([self.preprocess_text(item) for item in paper[key]])\n",
    "        \n",
    "        return paper\n",
    "\n",
    "    def extract_and_preprocess_papers(self, dir_path: str, output_file: str = 'preprocessed_papers.txt'):\n",
    "        cache_file = os.path.join(self.cache_dir, output_file)\n",
    "        if os.path.exists(cache_file):\n",
    "            logging.info(f\"Using cached preprocessed papers from {cache_file}\")\n",
    "            return cache_file\n",
    "\n",
    "        with open(cache_file, 'w', encoding='utf-8') as out_file:\n",
    "            for filename in tqdm(os.listdir(dir_path), desc=\"Preprocessing papers\"):\n",
    "                if filename.endswith('.xml'):\n",
    "                    try:\n",
    "                        with open(os.path.join(dir_path, filename), 'r', encoding='utf-8') as file:\n",
    "                            tree = ET.parse(file)\n",
    "                            root = tree.getroot()\n",
    "                            for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "                                title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
    "                                summary = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
    "                                full_text = title + '\\n\\n' + summary\n",
    "                                processed_paper = self.process_paper(full_text)\n",
    "                                out_file.write(json.dumps(processed_paper) + '\\n')\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing file {filename}: {e}\")\n",
    "        return cache_file\n",
    "\n",
    "    def train_model(self, train_file: str, output_dir: str = './custom_model', \n",
    "                    num_epochs: int = 10, batch_size: int = 8, \n",
    "                    learning_rate: float = 1e-4, max_length: int = 512):\n",
    "        train_dataset = PaperDataset(train_file, self.tokenizer, max_length)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "                inputs, masks = batch\n",
    "                inputs = inputs.to(self.device)\n",
    "                masks = masks.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs, mask=masks)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), inputs.view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            logging.info(f\"Epoch {epoch+1} - Loss: {epoch_loss/len(train_loader)}\")\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        torch.save(self.model.state_dict(), os.path.join(output_dir, 'model_weights.pth'))\n",
    "        logging.info(f\"Model saved to {output_dir}\")\n",
    "\n",
    "\n",
    "    def generate_scientific_paper(self, prompt: str, max_length: int = 2000, \n",
    "                                  num_return_sequences: int = 1, temperature: float = 0.7) -> str:\n",
    "        try:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "                temperature=temperature,\n",
    "                no_repeat_ngram_size=2,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating scientific paper: {e}\")\n",
    "            return None\n",
    "\n",
    "    def generate_sample_text(self, prompt: str, max_length: int = 100) -> str:\n",
    "        return self.generate_scientific_paper(prompt, max_length, num_return_sequences=1, temperature=0.7)\n",
    "\n",
    "    def refine_with_openai(self, draft: str) -> str:\n",
    "        if not openai.api_key:\n",
    "            raise ValueError(\"OpenAI API key is not set\")\n",
    "        \n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a scientific paper editor. Refine and improve the following draft:\"},\n",
    "                    {\"role\": \"user\", \"content\": draft}\n",
    "                ],\n",
    "                max_tokens=1000,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            return response.choices[0].message['content']\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error refining with OpenAI: {e}\")\n",
    "            return draft  # Return original draft if refinement fails\n",
    "\n",
    "    def format_paper(self, text: str) -> str:\n",
    "        sections = ['Abstract', 'Introduction', 'Methods', 'Results', 'Discussion', 'Conclusion']\n",
    "        formatted_paper = \"\"\n",
    "        for section in sections:\n",
    "            pattern = f\"{section}:?(.*?)(?={sections[sections.index(section)+1]}|$)\"\n",
    "            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "            if match:\n",
    "                formatted_paper += f\"\\n\\n{section}\\n{match.group(1).strip()}\"\n",
    "        return formatted_paper.strip()\n",
    "\n",
    "    def run_full_pipeline(self, query: str, num_papers: int = 10000, train_epochs: int = 10, dry_run: bool = False) -> Union[str, None]:\n",
    "        try:\n",
    "            # Step 1: Data Collection\n",
    "            logging.info(\"Starting paper collection...\")\n",
    "            papers_dir = os.path.join(self.cache_dir, query.replace(\" \", \"_\") + \"_papers\")\n",
    "            if not os.path.exists(papers_dir):\n",
    "                self.save_papers(query, papers_dir, max_papers=num_papers)\n",
    "            else:\n",
    "                logging.info(f\"Using cached papers from {papers_dir}\")\n",
    "\n",
    "            # Step 2: Data Preprocessing\n",
    "            logging.info(\"Starting data preprocessing...\")\n",
    "            train_file = self.extract_and_preprocess_papers(papers_dir)\n",
    "\n",
    "            # Step 3: Model Training\n",
    "            logging.info(\"Starting model training...\")\n",
    "            self.train_model(train_file, num_epochs=train_epochs)\n",
    "\n",
    "            if dry_run:\n",
    "                logging.info(\"Dry run: Generating sample text...\")\n",
    "                sample_text = self.generate_sample_text(f\"Write about {query}\")\n",
    "                logging.info(f\"Sample generated text: {sample_text}\")\n",
    "                return sample_text\n",
    "\n",
    "            # Step 4: Generate Paper\n",
    "            logging.info(\"Generating full paper...\")\n",
    "            prompt = f\"\"\"Write a comprehensive scientific paper about {query}. \n",
    "            Include the following sections:\n",
    "            1. Abstract\n",
    "            2. Introduction\n",
    "            3. Methods\n",
    "            4. Results\n",
    "            5. Discussion\n",
    "            6. Conclusion\n",
    "            Ensure each section is well-developed and follows scientific writing conventions.\"\"\"\n",
    "            \n",
    "            paper = self.generate_scientific_paper(prompt, max_length=3000)\n",
    "            if paper:\n",
    "                try:\n",
    "                    formatted_paper = self.format_paper(paper)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error formatting paper: {e}\")\n",
    "                    return None\n",
    "            else:\n",
    "                logging.error(\"Failed to generate paper.\")\n",
    "                return None\n",
    "\n",
    "            # Step 5: Refine Paper (if OpenAI API key is provided)\n",
    "            if openai.api_key:\n",
    "                try:\n",
    "                    logging.info(\"Refining paper with OpenAI...\")\n",
    "                    formatted_paper = self.refine_with_openai(formatted_paper)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error refining with OpenAI: {e}\")\n",
    "                    logging.info(\"Continuing with unrefined paper.\")\n",
    "\n",
    "            return formatted_paper\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in pipeline: {e}\")\n",
    "            return None\n",
    "\n",
    "    def save_model(self, path: str) -> None:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        torch.save(self.model.state_dict(), os.path.join(path, 'model_weights.pth'))\n",
    "        logging.info(f\"Model weights saved to {path}\")\n",
    "\n",
    "    def load_model(self, path: str) -> None:\n",
    "        self.model.load_state_dict(torch.load(os.path.join(path, 'model_weights.pth')))\n",
    "        self.model.to(self.device)\n",
    "        logging.info(f\"Model weights loaded from {path}\")\n",
    "\"\"\"\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    vocab_size = 10000\n",
    "    hidden_size = 256\n",
    "    num_layers = 4\n",
    "    num_heads = 8\n",
    "    dropout = 0.1\n",
    "\n",
    "    pipeline = CustomPaperPipeline(vocab_size, hidden_size, num_layers, num_heads, dropout, openai_api_key=openai_key)\n",
    "    \n",
    "    # Dry run to test the pipeline\n",
    "    sample_text = pipeline.run_full_pipeline('machine learning in healthcare', num_papers=1000, train_epochs=1, dry_run=True)\n",
    "    print(\"Sample text:\", sample_text)\n",
    "    \n",
    "    # Full run\n",
    "    paper = pipeline.run_full_pipeline('machine learning in healthcare', num_papers=10000, train_epochs=10)\n",
    "    if paper:\n",
    "        print(\"Generated paper:\")\n",
    "        print(paper)\n",
    "    else:\n",
    "        print(\"Failed to generate paper.\")\n",
    "\n",
    "    # Save the trained model\n",
    "    pipeline.save_model('./custom_trained_model')\n",
    "\n",
    "    # Load a previously trained model\n",
    "    pipeline.load_model('./custom_trained_model')\n",
    "\n",
    "    # Generate a new paper using the loaded model\n",
    "    new_paper = pipeline.generate_scientific_paper(\"The future of AI in healthcare\")\n",
    "    if new_paper:\n",
    "        print(\"New generated paper:\")\n",
    "        print(new_paper)\n",
    "    else:\n",
    "        print(\"Failed to generate new paper.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02487688-b176-4868-92cd-db2077923256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
